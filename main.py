import os
import logging
from dotenv import load_dotenv
from langfuse import get_client
from langfuse.openai import openai
from text import INPUT_TEXT, REFERENCE


load_dotenv()
os.environ["LANGFUSE_DEBUG"] = "True"
logging.getLogger("langfuse").setLevel(logging.DEBUG)

lf = get_client()
assert lf.auth_check(), "Langfuse creds / host invalid"

SYSTEM_PROMPT = "Summarize the following technical description in 3-4 sentences."

def generate_summary(prompt: str) -> str:
    # —Å–æ–∑–¥–∞—ë–º –∫–æ—Ä–Ω–µ–≤–æ–π span (trace) –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ summary
    with lf.start_as_current_span(
        name="summary-flow",
        input=prompt
    ) as root:

        # —Å–æ–∑–¥–∞—ë–º —Ñ–µ–π–∫–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é ‚Äî —è–∫–æ–±—ã —ç—Ç–∞–ª–æ–Ω–Ω–æ–µ summary –æ—Ç LLM (–¥–ª—è –±—É–¥—É—â–µ–π –æ—Ü–µ–Ω–∫–∏)
        with lf.start_as_current_generation(
            name="reference-summary-fake-gen",
            model="reference-placeholder",
            input=[{"role": "assistant", "content": REFERENCE}]
        ):
            pass  # –∑–¥–µ—Å—å –Ω–∏—á–µ–≥–æ –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –∫–∞–∫ –±—É–¥—Ç–æ "—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏" —ç—Ç–∞–ª–æ–Ω

        # —Å–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–π–¥—ë—Ç –≤ OpenAI
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ]

        # —Å–æ–∑–¥–∞—ë–º span –¥–ª—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –≤—ã–∑–æ–≤–∞ –º–æ–¥–µ–ª–∏
        with lf.start_as_current_generation(
            name="generate_summary",
            model="gpt-4o",
            input=messages,
        ) as gen:

            # –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ OpenAI –∏ –ø–æ–ª—É—á–∞–µ–º summary
            resp = openai.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.3,
            )
            summary = resp.choices[0].message.content.strip()

            # –ª–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Langfuse (output)
            gen.update(output=summary)

            # —Å—Ç–∞–≤–∏–º –º–µ—Ç–∫—É –¥–ª—è auto-eval: ¬´–æ—Ü–µ–Ω–∏—Ç—å –ø–æ–∑–∂–µ¬ª
            gen.score(
                name="summary-quality",
                value="pending",
                data_type="CATEGORICAL",
                comment="auto-eval pipeline",
            )

            # –≤—ã–≤–æ–¥–∏–º ID trace –∏ generation –≤ –∫–æ–Ω—Å–æ–ª—å
            print("ü™™ trace:", root.id)
            print("ü™™ generation:", gen.id)
            return summary



if __name__ == "__main__":
    summary = generate_summary(INPUT_TEXT.strip())
    lf.flush()
    print("\n‚úÖ Summary:\n", summary)
